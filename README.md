# MMoE Interpretation and Reproduction
论文题目：Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts
作者：Jiaqi Ma 等
发表会议：KDD 2018

一、概述

  在推荐系统中，排序模型往往需要同时优化多个目标，如点击率（CTR）、转化率（CVR）、用户停留时长或满意度等。这些任务之间既存在相关性，又可能出现冲突，如何有效建模任务关系是多任务学习中的关键问题。传统的共享结构（如 shared-bottom 模型）在一定程度上能够捕捉公共特征，但对任务间相关性和差异性敏感，容易造成负迁移。
  本文提出了 Multi-gate Mixture-of-Experts (MMoE) 模型，旨在通过灵活的专家选择机制提升多任务学习效果。作者在合成数据和真实数据场景中进行实验，结果表明 MMoE 在任务相关性较低或噪声较大的情况下，均显著优于 baseline 模型。


二、文章脉络

引言-综述-对baseline的合成实验- MMoE的建模方法-MMoE在合成数据上应用-在真实数据中的应用

三、主要内容梳理
1. 引言与 baseline 模型
  作者首先介绍了 shared-bottom 模型。该模型通过共享一个底层网络来提取输入特征，再为每个任务建立独立的上层结构（如 MLP 塔）。这种方式在任务相似时表现较好，但当任务分布差异较大时，模型性能显著下降。由此，作者提出了改进的方向，即用门控制的方法在共享和差异之间寻求更灵活的建模方式。

2. 合成数据实验
  为了系统分析任务相关性对模型性能的影响，作者设计了合成实验：
  随机生成正交向量并基于公式构造权重，通过余弦相似度衡量任务之间的相关性（经验证可替代皮尔逊相关系数）。
  实验结果表明：shared-bottom 模型对任务相关性高度敏感，任务相关度越高，模型效果越好；而在低相关度场景下性能明显下降。
  小结：该实验揭示了 baseline 方法在建模任务关系上的局限性，即对任务的相关度敏感，为后续提出 MMoE 做铺垫。

3. MMoE 模型建模方法
  在 shared-bottom 的基础上，作者引入了多个专家网络（experts）和任务特定的门控机制（multi-gates）：
    每个 expert 学习不同的特征模式；
    每个任务通过独立的 gate 来加权选择 experts；
  相比 OMoE（单一 gate 的 MoE），MMoE 能让不同任务根据需求独立地选择专家，更好地捕捉任务间的差异。
  小结：MMoE 在共享与独立之间取得平衡，有效缓解了负迁移问题。

四、合成数据上的 MMoE 实验
   
实验结果显示：
在任务高度相关时，MMoE 和 baseline 差异不大；
在低相关场景下，MMoE 显著优于 shared-bottom 与 OMoE。
并且，根据loss的直方图可以看出，MMoE的鲁棒性最强
结论：MMoE 的优势在于对低相关度任务有更强的建模能力。


五、将各种模型应用在真实数据中比较

1.在这部分增加了几个baseline以增强可信度。

L2-Constrained：两任务的跨语言问题，不同参数通过一个L2来软约束

Cross- Stitch：既要共享信息，又要独立性。

Tensor- Factorization：通过张量分解（此处为Tucker）实现参数共享


2.真实情景1:人口普查

多任务一：
Task1:预测收入是否超过50k
Task2:预测此人是否从来没有结婚
相似度：0.1768

多任务二：
Task1:预测此人学历是否高于大专
Task2:预测此人是否从来没有结婚
相似度：0.2373

移除教育，婚姻情况标签，验证集的目标是AUC

结论：MMoE的情况几乎都是最好的


3.真实情景2:大模型内容推荐

总目标：给定当前用户消费，预测其下一步消费（这里的消费指不同的交互）

优化排序模型目标：
Task1：参与度任务
Task2：满意度任务

实验设置：将特征系数矩阵归一稠密到[0,1]之间，MMoE在保留shared- Bottom共享层基础上添加

结果：满意度任务稀疏，噪声大，不用其AUC，结果都是MMoE好，且MMoE会改善baseline的结果


六、结论

通过合成实验和真实数据验证，MMoE 模型在处理多任务问题时相较 baseline 有明显优势，特别是在任务相关性较低或标签噪声较高的情况下，能够显著缓解负迁移效应，提升整体预测表现。


七、个人见解

优点：
灵活建模任务关系，每个任务拥有独立的门控机制，避免了过度共享带来的干扰；
在工业级推荐系统中具有实际可扩展性和应用价值。
启发：
未来可以尝试将 MMoE 与 attention 或 transformer 结构结合，以进一步提升任务间关系的动态建模能力。同时，对于数据不均衡或长尾任务，可以探索门控机制的正则化或重加权方法，以缓解大任务对小任务的压制。

